import os
import io
import time
import zipfile
import base64
import requests
import numpy as np
import PIL.Image
import streamlit as st
from openai import OpenAI
import cohere


# ------------ é…ç½®API KEY ------------
COHERE_API_KEY = os.getenv("COHERE_API_KEY", "ä½ çš„api key")
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY", "ä½ çš„api key")

# æ¨¡å‹é€‰ç”¨
EMBED_MODEL = "embed-v4.0"
QUERY_MODEL = "qwen3-235b-a22b"
VISION_MODEL = "qvq-max-2025-03-25"

#cssæ ·å¼
st.markdown(
    """
    <style>
    body {
    font-family: 'Orbitron', sans-serif;
    background: linear-gradient(135deg, #0d0d0d 0%, #1a1a1a 100%);
    color: #e6e6e6;
}

/* å®¹å™¨ç»ç’ƒåŒ– */
.main .block-container {
    background: rgba(255, 255, 255, 0.03);
    border-radius: 15px;
    box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.4);
    backdrop-filter: blur(10px);
    padding: 2rem;
}

/* ä¾§è¾¹æ  */
.sidebar .sidebar-content {
    width: 100%;
    background: radial-gradient(circle at 20% 20%, #1c1f26 0%, #0f1118 90%);
    padding: 1.5rem;
    border-right: 1px solid rgba(255, 255, 255, 0.05);
}

/* å¡ç‰‡ç»„ä»¶ */
.card {
    background: rgba(255, 255, 255, 0.05);
    border-radius: 12px;
    padding: 1.2rem;
    margin: 1rem 0;
    transition: all 0.3s ease-in-out;
    border: 1px solid rgba(255, 255, 255, 0.05);
    box-shadow: 0 4px 15px rgba(0, 255, 255, 0.05);
}

.card:hover {
    transform: translateY(-5px);
    box-shadow: 0 8px 20px rgba(0, 255, 255, 0.1);
}

/* æŒ‰é’®æ ·å¼ */
.stButton button {
    background: linear-gradient(45deg, #00c6ff, #0072ff);
    border: none;
    border-radius: 8px;
    padding: 0.6rem 1.5rem;
    font-weight: bold;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    box-shadow: 0 4px 15px rgba(0, 114, 255, 0.4);
    transition: all 0.3s ease;
}

.stButton button:hover {
    transform: scale(1.05);
    box-shadow: 0 6px 20px rgba(0, 114, 255, 0.6);
}

/* è¾“å…¥æ¡†æ ·å¼ */
.stTextInput input {
    background-color: rgba(255, 255, 255, 0.05);
    border: 1px solid rgba(0, 114, 255, 0.3);
    border-radius: 8px;
    padding: 0.5rem 1rem;
    color: #00ffff;
    box-shadow: inset 0 0 5px rgba(0, 255, 255, 0.2);
}

.stTextInput input:focus {
    outline: none;
    border-color: #0072ff;
    box-shadow: 0 0 8px rgba(0, 114, 255, 0.5);
}

/* å›¾åƒé¢„è§ˆå¢å¼º */
.image-preview {
    border: 2px dashed rgba(0, 114, 255, 0.3);
    border-radius: 10px;
    padding: 1rem;
    transition: all 0.3s ease;
}

.image-preview:hover {
    border-color: #00ffff;
    background-color: rgba(0, 114, 255, 0.05);
}

/* è‡ªå®šä¹‰åŠ è½½åŠ¨ç”» */
.loading-spinner {
    display: inline-block;
    width: 20px;
    height: 20px;
    border: 3px solid #00ffff;
    border-radius: 50%;
    border-top-color: transparent;
    animation: spin 1s ease-in-out infinite;
}

@keyframes spin {
    to { transform: rotate(360deg); }
}

/* å“åº”å¼ç½‘æ ¼ */
@media (min-width: 768px) {
    .grid-container {
        display: grid;
        grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
        gap: 1.5rem;
    }
}
</style>
    """ , unsafe_allow_html=True)



# åˆå§‹åŒ–å®¢æˆ·ç«¯
def init_clients():
    co = cohere.ClientV2(api_key=COHERE_API_KEY)
    qwen_client = OpenAI(
        api_key=DASHSCOPE_API_KEY,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    return co, qwen_client

co, qwen_client = init_clients()

MAX_PIXELS = 1568 * 1568
SIM_THRESHOLD = 0.3  # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
# ------------ä¸€äº›åŠŸèƒ½æ€§å‡½æ•° ------------
def resize_image(pil_image: PIL.Image.Image):
    w, h = pil_image.size
    if w * h > MAX_PIXELS:
        scale = (MAX_PIXELS / (w * h)) ** 0.5
        pil_image.thumbnail((int(w * scale), int(h * scale)))


def base64_from_pil(pil_image: PIL.Image.Image) -> str:
    fmt = pil_image.format or "PNG"
    resize_image(pil_image)
    with io.BytesIO() as buf:
        pil_image.save(buf, format=fmt)
        data = base64.b64encode(buf.getvalue()).decode()
    return f"data:image/{fmt.lower()};base64,{data}"



def embed_document(img: PIL.Image.Image):
    b64 = base64_from_pil(img)
    doc_in = {"content": [{"type": "image", "image": b64}]}
    resp = co.embed(model=EMBED_MODEL,
                    input_type="search_document",
                    embedding_types=["float"],
                    inputs=[doc_in])
    vec = np.array(resp.embeddings.float[0])
    return vec / np.linalg.norm(vec)


def embed_query(text: str):
    resp = co.embed(model=EMBED_MODEL,
                    input_type="search_query",
                    embedding_types=["float"],
                    texts=[text])
    vec = np.array(resp.embeddings.float[0])
    return vec / np.linalg.norm(vec)




def generate_answer(question: str, img: PIL.Image.Image = None):
    """æ”¯æŒæµå¼è¾“å‡º"""
    if img is not None:
        # å¤„ç†å›¾æ–‡è¾“å…¥ï¼ˆä½¿ç”¨æµå¼æ¨¡å¼ï¼‰
        b64_image = base64_from_pil(img)

        messages = [
            {"role": "system", "content": "æ ¹æ®ä¸‹é¢çš„å›¾ç‰‡å›ç­”é—®é¢˜ã€‚"},
             {
            "role": "user",
            "content": [
                {"type": "text", "text": question},
                {"type": "image_url", "image_url": {"url": b64_image}}
            ]
        }]
        response = qwen_client.chat.completions.create(
            model=VISION_MODEL,
            messages=messages,
            stream=True,  # å¯ç”¨æµå¼è¾“å‡º

        )
    else:
        # å¤„ç†çº¯æ–‡æœ¬è¾“å…¥
        messages = [
            {"role": "system", "content": "è¯·ä»…æ ¹æ®æ–‡å­—é—®é¢˜å›ç­”"},
            {"role": "user", "content": question}
        ]
        response = qwen_client.chat.completions.create(
            model=QUERY_MODEL,
            messages=messages,
            stream=True,  # å¯ç”¨æµå¼è¾“å‡º

        )


    def stream_response():
        try:
            for chunk in response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            st.error(f"æµå¼è¾“å‡ºé”™è¯¯: {str(e)}")
            yield "æŠ±æ­‰ï¼Œæµå¼è¾“å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ã€‚"

    return stream_response()

# ------------ ä¼šè¯çŠ¶æ€ ------------
if 'img_paths' not in st.session_state:
    st.session_state.img_paths = []
if 'doc_embeddings' not in st.session_state:
    st.session_state.doc_embeddings = []


st.sidebar.title("å¯¼èˆªæ ")
page = st.sidebar.radio("åŠŸèƒ½", ["æŸ¥è¯¢", "ä¸Šä¼ ", "å›¾åº“"], index=0)

# ------------ ä¸Šä¼ é¡µé¢ ------------
if page == "ä¸Šä¼ ":
    st.header("ä¸Šä¼ å›¾ç‰‡èµ„æº")
    uploaded = st.file_uploader("ä¸Šä¼ å•å¼ å›¾ç‰‡", type=['png','jpg','jpeg'])
    url = st.text_input("æˆ–è¾“å…¥å›¾ç‰‡ URL")
    zipf = st.file_uploader("ä¸Šä¼  ZIP å‹ç¼©åŒ… (ä»…å›¾ç‰‡)", type=['zip'])

    if uploaded:
        img = PIL.Image.open(uploaded)
        path = f"uploaded_{int(time.time())}.png"
        img.save(path)
        st.session_state.img_paths.append(path)
        st.session_state.doc_embeddings.append(embed_document(img))
        st.success(f"å·²æ·»åŠ : {path}")
    if url:
        try:
            r = requests.get(url)
            r.raise_for_status()
            img = PIL.Image.open(io.BytesIO(r.content))
            path = f"url_{int(time.time())}.png"
            img.save(path)
            st.session_state.img_paths.append(path)
            st.session_state.doc_embeddings.append(embed_document(img))
            st.success(f"å·²ä¸‹è½½å¹¶æ·»åŠ æˆåŠŸ: {path}")
        except Exception as e:
            st.error(f"URL ä¸‹è½½å¤±è´¥ğŸ‘€: {e}")
    if zipf:
        with zipfile.ZipFile(zipf) as z:
            for fname in z.namelist():
                if fname.lower().endswith(('.png','.jpg','.jpeg')):
                    data = z.read(fname)
                    img = PIL.Image.open(io.BytesIO(data))
                    path = f"zip_{int(time.time())}_{os.path.basename(fname)}"
                    img.save(path)
                    st.session_state.img_paths.append(path)
                    st.session_state.doc_embeddings.append(embed_document(img))
            st.success("ZIP ä¸­çš„æ‰€æœ‰å›¾ç‰‡å·²æ·»åŠ ")

# ------------ æŸ¥è¯¢é¡µé¢ ------------
elif page == "æŸ¥è¯¢":
    st.header("ğŸ” å›¾åƒRAG-æ–‡å­—æ£€ç´¢å›¾ç‰‡")
    question = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜âœï¸ï¼š")
    if st.button("æ£€ç´¢å¹¶å›ç­”"):
        if not question:
            st.warning("è¯·è¾“å…¥é—®é¢˜åå†æ£€ç´¢ğŸ¤“ğŸ‘†")
        else:
            q_emb = embed_query(question)
            if st.session_state.doc_embeddings:
                docs = np.vstack(st.session_state.doc_embeddings)
                sims = docs.dot(q_emb)
                max_sim = float(np.max(sims))
                idx = int(np.argmax(sims))
            else:
                max_sim = 0
            answer_container = st.empty()
            full_answer = ""
            # åˆ¤æ–­æ˜¯å¦å‘½ä¸­
            if max_sim < SIM_THRESHOLD:
            # æ–‡å­—å›ç­”æµå¼æ¸²æŸ“
              ans_generator = generate_answer(question)
              for token in ans_generator:
                full_answer += token
                answer_container.markdown(f"**æ¨¡å‹æ–‡å­—å›ç­”ï¼ˆæ— ç›¸å…³å›¾ç‰‡ï¼‰ï¼š** {full_answer}")
            else:

            # æ˜¾ç¤ºå›¾ç‰‡
              best = st.session_state.img_paths[idx]
              img = PIL.Image.open(best)
              st.image(img, caption=f"æœ€ç›¸å…³å›¾ç‰‡ï¼ˆç›¸å…³åº¦ï¼š{max_sim:.2f}ï¼‰", use_column_width=True)

            # å›¾æ–‡å›ç­”æµå¼æ¸²æŸ“
              ans_generator = generate_answer(question, img)
              for token in ans_generator:
                full_answer += token
                answer_container.markdown(f"**æ¨¡å‹å›ç­”ï¼š** {full_answer}")

# ------------ å›¾åº“é¡µé¢ ------------
elif page == "å›¾åº“":
    st.header("ğŸ“¸ å·²å½•å…¥å›¾ç‰‡å›¾åº“")
    paths = st.session_state.img_paths
    if not paths:
        st.info("""
å•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šï¼
å®å®è‚šè‚šæ‰“é›·å•¦ğŸ˜¡ã€‚
""")
        st.info("""ä¸€å¼ å›¾ç‰‡éƒ½æ²¡æœ‰
**â—ï¸è¯·å…ˆåœ¨â€œä¸Šä¼ â€é¡µé¢æ·»åŠ å›¾ç‰‡â—ï¸**""")
        st.info("""å•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šï¼""")
    else:
        for i in range(0, len(paths), 3):
            cols = st.columns(3)
            for j, path in enumerate(paths[i:i+3]):
                with cols[j]:
                    img = PIL.Image.open(path)
                    st.image(img, width=150, caption=os.path.basename(path))